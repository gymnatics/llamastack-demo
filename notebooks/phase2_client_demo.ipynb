{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LlamaStack Client Integration Demo\n",
        "\n",
        "This notebook demonstrates how to integrate with LlamaStack Distribution on OpenShift AI.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "1. **Connect to LlamaStack** - Initialize the client\n",
        "2. **List Available Models** - See vLLM and Azure OpenAI providers\n",
        "3. **List MCP Servers** - Discover available tools\n",
        "4. **Switch Providers** - Change from vLLM to Azure OpenAI\n",
        "5. **Use MCP Tools** - Call tools through agents\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- LlamaStack Distribution deployed on OpenShift\n",
        "- Access to the LlamaStack endpoint\n",
        "- `llama-stack-client` Python package installed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, let's install the required packages and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q llama-stack-client requests python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "from typing import List, Dict\n",
        "\n",
        "# LlamaStack endpoint - update this to your deployment\n",
        "# For OpenShift internal access:\n",
        "LLAMASTACK_URL = os.getenv(\"LLAMASTACK_URL\", \"http://lsd-genai-playground-service.my-first-model.svc.cluster.local:8321\")\n",
        "\n",
        "# For external access (if you have a route):\n",
        "# LLAMASTACK_URL = \"https://llamastack-route.apps.your-cluster.com\"\n",
        "\n",
        "print(f\"LlamaStack URL: {LLAMASTACK_URL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Connect to LlamaStack\n",
        "\n",
        "Let's verify we can connect to the LlamaStack server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_health():\n",
        "    \"\"\"Check if LlamaStack is healthy.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f\"{LLAMASTACK_URL}/v1/health\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(\"‚úÖ LlamaStack is healthy!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ùå Health check failed: {response.status_code}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Connection error: {e}\")\n",
        "        return False\n",
        "\n",
        "check_health()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. List Available Models\n",
        "\n",
        "LlamaStack can have multiple inference providers. Let's see what models are available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_models() -> List[Dict]:\n",
        "    \"\"\"Fetch all available models from LlamaStack.\"\"\"\n",
        "    response = requests.get(f\"{LLAMASTACK_URL}/v1/models\", timeout=10)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        return data.get(\"data\", [])\n",
        "    return []\n",
        "\n",
        "models = get_models()\n",
        "\n",
        "# Filter to LLM models only\n",
        "llm_models = [m for m in models if m.get(\"model_type\") == \"llm\"]\n",
        "\n",
        "print(f\"üìä Total models available: {len(models)}\")\n",
        "print(f\"ü§ñ LLM models: {len(llm_models)}\")\n",
        "print()\n",
        "\n",
        "# Group by provider\n",
        "providers = {}\n",
        "for m in llm_models:\n",
        "    provider = m.get(\"provider_id\", \"unknown\")\n",
        "    if provider not in providers:\n",
        "        providers[provider] = []\n",
        "    providers[provider].append(m.get(\"identifier\", m.get(\"model_id\")))\n",
        "\n",
        "print(\"Models by Provider:\")\n",
        "print(\"=\" * 50)\n",
        "for provider, model_list in providers.items():\n",
        "    print(f\"\\nüîπ {provider} ({len(model_list)} models)\")\n",
        "    # Show first 5 models\n",
        "    for model in model_list[:5]:\n",
        "        print(f\"   ‚Ä¢ {model}\")\n",
        "    if len(model_list) > 5:\n",
        "        print(f\"   ... and {len(model_list) - 5} more\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. List MCP Servers (Tools)\n",
        "\n",
        "MCP (Model Context Protocol) servers provide tools that the LLM can use. Let's see what's available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_tools() -> List[Dict]:\n",
        "    \"\"\"Fetch all available tools from LlamaStack.\"\"\"\n",
        "    response = requests.get(f\"{LLAMASTACK_URL}/v1/tools\", timeout=10)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        if isinstance(data, list):\n",
        "            return data\n",
        "        return data.get(\"data\", [])\n",
        "    return []\n",
        "\n",
        "tools = get_tools()\n",
        "\n",
        "print(f\"üõ†Ô∏è Total tools available: {len(tools)}\")\n",
        "print()\n",
        "\n",
        "# Group by toolgroup (MCP server)\n",
        "toolgroups = {}\n",
        "for t in tools:\n",
        "    tg = t.get(\"toolgroup_id\", \"unknown\")\n",
        "    if tg not in toolgroups:\n",
        "        toolgroups[tg] = []\n",
        "    toolgroups[tg].append(t.get(\"name\", \"unknown\"))\n",
        "\n",
        "print(\"MCP Servers (Tool Groups):\")\n",
        "print(\"=\" * 50)\n",
        "for tg, tool_list in sorted(toolgroups.items()):\n",
        "    icon = \"üå§Ô∏è\" if \"weather\" in tg else \"üë•\" if \"hr\" in tg else \"üìã\" if \"jira\" in tg else \"üêô\" if \"github\" in tg else \"üîß\"\n",
        "    print(f\"\\n{icon} {tg} ({len(tool_list)} tools)\")\n",
        "    for tool in tool_list:\n",
        "        print(f\"   ‚Ä¢ {tool}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Chat Completion with Different Providers\n",
        "\n",
        "Now let's demonstrate switching between providers. We'll use the OpenAI-compatible API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_completion(model_id: str, message: str, tools: List[Dict] = None) -> Dict:\n",
        "    \"\"\"Send a chat completion request to LlamaStack.\"\"\"\n",
        "    payload = {\n",
        "        \"model\": model_id,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant with access to various tools.\"},\n",
        "            {\"role\": \"user\", \"content\": message}\n",
        "        ],\n",
        "        \"temperature\": 0.7,\n",
        "        \"max_tokens\": 1024\n",
        "    }\n",
        "    \n",
        "    if tools:\n",
        "        payload[\"tools\"] = tools\n",
        "        payload[\"tool_choice\"] = \"auto\"\n",
        "    \n",
        "    response = requests.post(\n",
        "        f\"{LLAMASTACK_URL}/v1/openai/v1/chat/completions\",\n",
        "        json=payload,\n",
        "        timeout=60\n",
        "    )\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        return {\"error\": f\"Status {response.status_code}: {response.text}\"}\n",
        "\n",
        "print(\"Chat completion function ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Using Local vLLM (Llama 3.2-3B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use local vLLM model\n",
        "VLLM_MODEL = \"vllm-inference/llama-32-3b-instruct\"\n",
        "\n",
        "print(f\"ü§ñ Using model: {VLLM_MODEL}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "response = chat_completion(\n",
        "    model_id=VLLM_MODEL,\n",
        "    message=\"What is the capital of France? Answer in one sentence.\"\n",
        ")\n",
        "\n",
        "if \"error\" in response:\n",
        "    print(f\"‚ùå Error: {response['error']}\")\n",
        "else:\n",
        "    content = response.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "    print(f\"\\nüìù Response from vLLM (Llama 3.2-3B):\")\n",
        "    print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Using Azure OpenAI (GPT-4.1-mini)\n",
        "\n",
        "Now let's switch to Azure OpenAI - just change the model ID!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Switch to Azure OpenAI - just change the model ID!\n",
        "AZURE_MODEL = \"azure-openai/gpt-4.1-mini\"\n",
        "\n",
        "print(f\"ü§ñ Using model: {AZURE_MODEL}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "response = chat_completion(\n",
        "    model_id=AZURE_MODEL,\n",
        "    message=\"What is the capital of France? Answer in one sentence.\"\n",
        ")\n",
        "\n",
        "if \"error\" in response:\n",
        "    print(f\"‚ùå Error: {response['error']}\")\n",
        "else:\n",
        "    content = response.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "    print(f\"\\nüìù Response from Azure OpenAI (GPT-4.1-mini):\")\n",
        "    print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Using MCP Tools\n",
        "\n",
        "Let's test the MCP servers by invoking tools directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def invoke_tool(tool_name: str, kwargs: Dict) -> str:\n",
        "    \"\"\"Invoke a tool directly via LlamaStack.\"\"\"\n",
        "    response = requests.post(\n",
        "        f\"{LLAMASTACK_URL}/v1/tool-runtime/invoke\",\n",
        "        json={\"tool_name\": tool_name, \"kwargs\": kwargs},\n",
        "        timeout=30\n",
        "    )\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        content = result.get(\"content\", [])\n",
        "        if isinstance(content, list) and content:\n",
        "            return content[0].get(\"text\", str(content))\n",
        "        return str(result)\n",
        "    else:\n",
        "        return f\"Error: {response.status_code}\"\n",
        "\n",
        "# Test HR MCP - List employees\n",
        "print(\"üë• HR MCP Server - List Employees\")\n",
        "print(\"=\" * 50)\n",
        "result = invoke_tool(\"list_employees\", {})\n",
        "print(result[:1000])  # Show first 1000 chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test GitHub MCP - Search repositories\n",
        "print(\"üêô GitHub MCP Server - Search Repositories\")\n",
        "print(\"=\" * 50)\n",
        "result = invoke_tool(\"search_repositories\", {\"query\": \"llamastack\"})\n",
        "print(result[:1500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Jira MCP - List projects\n",
        "print(\"üìã Jira/Confluence MCP Server - List Projects\")\n",
        "print(\"=\" * 50)\n",
        "result = invoke_tool(\"list_projects\", {})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary\n",
        "\n",
        "In this notebook, we demonstrated:\n",
        "\n",
        "1. ‚úÖ **Connecting to LlamaStack** - Simple HTTP API\n",
        "2. ‚úÖ **Listing Models** - Both vLLM and Azure OpenAI providers  \n",
        "3. ‚úÖ **Listing MCP Servers** - Weather, HR, Jira, GitHub tools\n",
        "4. ‚úÖ **Switching Providers** - Just change the `model_id`!\n",
        "5. ‚úÖ **Using MCP Tools** - Direct invocation\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **Provider Switching is Easy**: Just change the model ID from `vllm-inference/llama-32-3b-instruct` to `azure-openai/gpt-4.1-mini`\n",
        "- **MCP Tools are Unified**: All tools are accessible through the same API regardless of which MCP server provides them\n",
        "- **OpenAI-Compatible API**: Use familiar OpenAI SDK patterns with LlamaStack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"üìä LlamaStack Configuration Summary\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\nüîó Endpoint: {LLAMASTACK_URL}\")\n",
        "print(f\"\\nü§ñ Inference Providers:\")\n",
        "for provider in providers.keys():\n",
        "    print(f\"   ‚Ä¢ {provider}\")\n",
        "print(f\"\\nüõ†Ô∏è MCP Servers:\")\n",
        "for tg in toolgroups.keys():\n",
        "    print(f\"   ‚Ä¢ {tg}\")\n",
        "print(f\"\\nüìà Total: {len(llm_models)} LLM models, {len(tools)} tools\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
