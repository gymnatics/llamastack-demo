{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîê Admin Demo: Azure OpenAI Integration\n",
        "\n",
        "**‚ö†Ô∏è ADMIN ONLY - Do not share this notebook with workshop participants**\n",
        "\n",
        "This notebook demonstrates how LlamaStack can abstract multiple inference providers.\n",
        "\n",
        "## What You'll Demonstrate\n",
        "1. LlamaStack with 2 LLM providers (vLLM + Azure OpenAI)\n",
        "2. Switching between local and cloud models with the same API\n",
        "3. The power of provider abstraction - clients don't need to change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Configuration - UPDATE THIS with your admin namespace!\n",
        "ADMIN_NAMESPACE = \"admin-workshop\"  # <-- Change to your admin namespace\n",
        "\n",
        "# LlamaStack endpoint (internal OpenShift service)\n",
        "LLAMASTACK_URL = f\"http://lsd-genai-playground-service.{ADMIN_NAMESPACE}.svc.cluster.local:8321\"\n",
        "\n",
        "print(f\"Admin Namespace: {ADMIN_NAMESPACE}\")\n",
        "print(f\"LlamaStack URL: {LLAMASTACK_URL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Show Available Models (2 Providers)\n",
        "\n",
        "After applying the Phase 2 config, LlamaStack should have 2 LLM models:\n",
        "- `llama-32-3b-instruct` (local vLLM)\n",
        "- `gpt-4.1-mini` (Azure OpenAI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = requests.get(f\"{LLAMASTACK_URL}/v1/models\", timeout=10)\n",
        "models = response.json().get(\"data\", [])\n",
        "\n",
        "# Filter to LLM models only\n",
        "llm_models = [m for m in models if m.get(\"model_type\") == \"llm\"]\n",
        "\n",
        "print(f\"ü§ñ LLM Models Available: {len(llm_models)}\")\n",
        "print(\"=\" * 60)\n",
        "for m in llm_models:\n",
        "    provider = m.get('provider_id')\n",
        "    icon = \"‚òÅÔ∏è\" if \"azure\" in provider.lower() else \"üñ•Ô∏è\"\n",
        "    print(f\"  {icon} {m.get('identifier')}\")\n",
        "    print(f\"     Provider: {provider}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Compare: Local vLLM vs Azure OpenAI\n",
        "\n",
        "Let's ask the same question to both models and compare responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define model IDs\n",
        "VLLM_MODEL = \"vllm-inference/llama-32-3b-instruct\"  # Local model\n",
        "AZURE_MODEL = \"azure-openai/gpt-4.1-mini\"  # Azure OpenAI\n",
        "\n",
        "# Test question\n",
        "TEST_QUESTION = \"What are the key benefits of using AI in healthcare? List 3 points briefly.\"\n",
        "\n",
        "print(f\"üìù Question: {TEST_QUESTION}\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query LOCAL vLLM model\n",
        "print(\"\\nüñ•Ô∏è LOCAL MODEL (vLLM - Llama 3.2-3B)\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "payload = {\n",
        "    \"model\": VLLM_MODEL,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": TEST_QUESTION}\n",
        "    ],\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 300\n",
        "}\n",
        "\n",
        "response = requests.post(\n",
        "    f\"{LLAMASTACK_URL}/v1/openai/v1/chat/completions\",\n",
        "    json=payload,\n",
        "    timeout=60\n",
        ")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    content = result.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "    print(content)\n",
        "else:\n",
        "    print(f\"‚ùå Error: {response.status_code} - {response.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query AZURE OpenAI model\n",
        "print(\"\\n‚òÅÔ∏è CLOUD MODEL (Azure OpenAI - GPT-4.1-mini)\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "payload = {\n",
        "    \"model\": AZURE_MODEL,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": TEST_QUESTION}\n",
        "    ],\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 300\n",
        "}\n",
        "\n",
        "response = requests.post(\n",
        "    f\"{LLAMASTACK_URL}/v1/openai/v1/chat/completions\",\n",
        "    json=payload,\n",
        "    timeout=60\n",
        ")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    content = result.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "    print(content)\n",
        "else:\n",
        "    print(f\"‚ùå Error: {response.status_code} - {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Key Takeaways for Participants\n",
        "\n",
        "**Show this summary to participants:**\n",
        "\n",
        "| Aspect | What Changed | What Stayed Same |\n",
        "|--------|--------------|------------------|\n",
        "| Config | Added Azure provider to ConfigMap | LlamaStack API unchanged |\n",
        "| Code | Just change `model` parameter | Same endpoint, same format |\n",
        "| Secrets | Azure keys in admin namespace only | Users never see API keys |\n",
        "\n",
        "**The Power of LlamaStack:**\n",
        "- ‚úÖ Same API for local and cloud models\n",
        "- ‚úÖ Switch providers by changing one parameter\n",
        "- ‚úÖ Secrets managed centrally by admin\n",
        "- ‚úÖ No code changes needed in client applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. (Optional) Show Tool Calling with Azure\n",
        "\n",
        "Demonstrate that Azure OpenAI can also use the MCP tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List available tools\n",
        "response = requests.get(f\"{LLAMASTACK_URL}/v1/tools\", timeout=10)\n",
        "data = response.json()\n",
        "tools = data if isinstance(data, list) else data.get(\"data\", [])\n",
        "\n",
        "mcp_tools = [t for t in tools if t.get(\"toolgroup_id\", \"\").startswith(\"mcp::\")]\n",
        "print(f\"üõ†Ô∏è MCP Tools Available: {len(mcp_tools)}\")\n",
        "for t in mcp_tools:\n",
        "    print(f\"  ‚Ä¢ {t.get('toolgroup_id')}/{t.get('name')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Azure OpenAI with tool calling (if tools are available)\n",
        "print(\"‚òÅÔ∏è Azure OpenAI with MCP Tools\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "payload = {\n",
        "    \"model\": AZURE_MODEL,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant with access to weather and HR tools.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}\n",
        "    ],\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(\n",
        "    f\"{LLAMASTACK_URL}/v1/openai/v1/chat/completions\",\n",
        "    json=payload,\n",
        "    timeout=60\n",
        ")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    content = result.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "    print(content)\n",
        "else:\n",
        "    print(f\"‚ùå Error: {response.status_code} - {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìã Admin Setup Reminder\n",
        "\n",
        "Before running this demo, ensure you have:\n",
        "\n",
        "1. **Created Azure OpenAI secret:**\n",
        "```bash\n",
        "oc create secret generic azure-openai-secret \\\n",
        "  --from-literal=endpoint=\"https://YOUR-RESOURCE.openai.azure.com/\" \\\n",
        "  --from-literal=api-key=\"YOUR-API-KEY\" \\\n",
        "  --from-literal=api-version=\"2024-12-01-preview\" \\\n",
        "  -n admin-workshop\n",
        "```\n",
        "\n",
        "2. **Applied Phase 2 LlamaStack config:**\n",
        "```bash\n",
        "oc apply -f manifests/llamastack/llama-stack-config-phase2.yaml -n admin-workshop\n",
        "oc delete pod -l app=lsd-genai-playground -n admin-workshop\n",
        "```\n",
        "\n",
        "3. **Verified 2 models are available:**\n",
        "```bash\n",
        "oc exec deployment/lsd-genai-playground -n admin-workshop -- \\\n",
        "  curl -s http://localhost:8321/v1/models | python3 -c \"\n",
        "import json,sys\n",
        "data=json.load(sys.stdin)\n",
        "llms=[m for m in data.get('data',[]) if m.get('model_type')=='llm']\n",
        "print(f'LLM Models: {len(llms)}')\n",
        "for m in llms:\n",
        "    print(f\\\"  - {m.get('identifier')} ({m.get('provider_id')})\\\")\n",
        "\"\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
