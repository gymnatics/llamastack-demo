{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LlamaStack Client Demo - Phase 1\n",
        "\n",
        "## Phase 1 Configuration\n",
        "- **1 Inference Provider**: vLLM (Llama 3.2-3B)\n",
        "- **1 MCP Server**: Weather only\n",
        "\n",
        "This notebook demonstrates client integration with a minimal LlamaStack distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "from typing import List, Dict\n",
        "\n",
        "# LlamaStack endpoint (internal OpenShift service)\n",
        "LLAMASTACK_URL = \"http://lsd-genai-playground-service.my-first-model.svc.cluster.local:8321\"\n",
        "\n",
        "print(f\"LlamaStack URL: {LLAMASTACK_URL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. List Available Models\n",
        "\n",
        "In Phase 1, we should see **only 1 LLM model** (vLLM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = requests.get(f\"{LLAMASTACK_URL}/v1/models\", timeout=10)\n",
        "models = response.json().get(\"data\", [])\n",
        "\n",
        "# Filter to LLM models only\n",
        "llm_models = [m for m in models if m.get(\"model_type\") == \"llm\"]\n",
        "\n",
        "print(f\"ü§ñ LLM Models Available: {len(llm_models)}\")\n",
        "print(\"=\" * 50)\n",
        "for m in llm_models:\n",
        "    print(f\"  ‚Ä¢ {m.get('identifier')} ({m.get('provider_id')})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. List MCP Servers (Tools)\n",
        "\n",
        "In Phase 1, we should see **only Weather MCP**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = requests.get(f\"{LLAMASTACK_URL}/v1/tools\", timeout=10)\n",
        "data = response.json()\n",
        "tools = data if isinstance(data, list) else data.get(\"data\", [])\n",
        "\n",
        "# Group by toolgroup (MCP server)\n",
        "toolgroups = {}\n",
        "for t in tools:\n",
        "    tg = t.get(\"toolgroup_id\", \"unknown\")\n",
        "    if tg not in toolgroups:\n",
        "        toolgroups[tg] = []\n",
        "    toolgroups[tg].append(t.get(\"name\", \"unknown\"))\n",
        "\n",
        "# Count MCP servers (exclude builtin)\n",
        "mcp_servers = [tg for tg in toolgroups.keys() if tg.startswith(\"mcp::\")]\n",
        "\n",
        "print(f\"üõ†Ô∏è MCP Servers: {len(mcp_servers)}\")\n",
        "print(f\"üìä Total Tools: {len(tools)}\")\n",
        "print(\"=\" * 50)\n",
        "for tg, tool_list in sorted(toolgroups.items()):\n",
        "    icon = \"üå§Ô∏è\" if \"weather\" in tg else \"üîß\"\n",
        "    print(f\"\\n{icon} {tg} ({len(tool_list)} tools)\")\n",
        "    for tool in tool_list:\n",
        "        print(f\"   ‚Ä¢ {tool}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Test Chat Completion with vLLM\n",
        "\n",
        "Using the **only available model**: vLLM (Llama 3.2-3B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the only available model: vLLM\n",
        "MODEL_ID = \"vllm-inference/llama-32-3b-instruct\"\n",
        "\n",
        "payload = {\n",
        "    \"model\": MODEL_ID,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is the capital of France? Answer in one sentence.\"}\n",
        "    ],\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 256\n",
        "}\n",
        "\n",
        "print(f\"ü§ñ Using model: {MODEL_ID}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "response = requests.post(\n",
        "    f\"{LLAMASTACK_URL}/v1/openai/v1/chat/completions\",\n",
        "    json=payload,\n",
        "    timeout=60\n",
        ")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    content = result.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "    print(f\"\\nüìù Response from vLLM (Llama 3.2-3B):\")\n",
        "    print(content)\n",
        "else:\n",
        "    print(f\"‚ùå Error: {response.status_code} - {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 1 Summary\n",
        "\n",
        "| Component | Count | Details |\n",
        "|-----------|-------|---------|\n",
        "| **LLM Models** | 1 | vLLM (Llama 3.2-3B) |\n",
        "| **MCP Servers** | 1 | Weather |\n",
        "| **Total Tools** | 3 | 2 RAG + 1 Weather |\n",
        "\n",
        "---\n",
        "\n",
        "### Next Step\n",
        "\n",
        "After the admin applies the **Phase 2 configuration**, run the **phase2_client_demo.ipynb** notebook to see:\n",
        "- 2 LLM models (vLLM + Azure OpenAI)\n",
        "- 3 MCP servers (Weather + HR + Jira)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
