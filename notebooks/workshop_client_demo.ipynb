{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LlamaStack Workshop - Client Demo\n",
        "\n",
        "This notebook demonstrates how to interact with your deployed LlamaStack distribution programmatically.\n",
        "\n",
        "## Prerequisites\n",
        "- You have deployed a model (Llama 3.2-3B) in your project\n",
        "- You have enabled the GenAI Playground (which creates a LlamaStack Distribution)\n",
        "- MCP servers have been configured in your LlamaStack\n",
        "\n",
        "## What You'll Learn\n",
        "1. How to list available models\n",
        "2. How to list available tools (MCP servers)\n",
        "3. How to make chat completions\n",
        "4. **How to invoke MCP tools directly**\n",
        "5. **How to use agent-based tool calling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Configuration - UPDATE THIS with your project name!\n",
        "PROJECT_NAME = \"user-XX\"  # <-- Change XX to your user number (e.g., user-05)\n",
        "\n",
        "# LlamaStack endpoint (internal OpenShift service)\n",
        "LLAMASTACK_URL = f\"http://lsd-genai-playground-service.{PROJECT_NAME}.svc.cluster.local:8321\"\n",
        "\n",
        "print(f\"Project: {PROJECT_NAME}\")\n",
        "print(f\"LlamaStack URL: {LLAMASTACK_URL}\")\n",
        "\n",
        "if PROJECT_NAME == \"user-XX\":\n",
        "    print(\"\\n‚ö†Ô∏è WARNING: Please update PROJECT_NAME above with your actual project name!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. List Available Models\n",
        "\n",
        "Let's see what models are available in your LlamaStack distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = requests.get(f\"{LLAMASTACK_URL}/v1/models\", timeout=10)\n",
        "models = response.json().get(\"data\", [])\n",
        "\n",
        "# Filter to LLM models only\n",
        "llm_models = [m for m in models if m.get(\"model_type\") == \"llm\"]\n",
        "\n",
        "print(f\"ü§ñ LLM Models Available: {len(llm_models)}\")\n",
        "print(\"=\" * 50)\n",
        "for m in llm_models:\n",
        "    print(f\"  ‚Ä¢ {m.get('identifier')} ({m.get('provider_id')})\")\n",
        "    \n",
        "# Store the first model ID for later use\n",
        "# Note: identifier already includes the provider prefix (e.g., \"vllm-inference-1/llama-32-3b-instruct\")\n",
        "if llm_models:\n",
        "    MODEL_ID = llm_models[0].get('identifier')\n",
        "    print(f\"\\nüìå Using model: {MODEL_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. List Available Tools (MCP Servers)\n",
        "\n",
        "Tools are provided by MCP servers. Let's see what's available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = requests.get(f\"{LLAMASTACK_URL}/v1/tools\", timeout=10)\n",
        "data = response.json()\n",
        "tools = data if isinstance(data, list) else data.get(\"data\", [])\n",
        "\n",
        "# Group by toolgroup (MCP server)\n",
        "toolgroups = {}\n",
        "for t in tools:\n",
        "    tg = t.get(\"toolgroup_id\", \"unknown\")\n",
        "    if tg not in toolgroups:\n",
        "        toolgroups[tg] = []\n",
        "    toolgroups[tg].append(t.get(\"name\", \"unknown\"))\n",
        "\n",
        "# Count MCP servers (exclude builtin)\n",
        "mcp_servers = [tg for tg in toolgroups.keys() if tg.startswith(\"mcp::\")]\n",
        "\n",
        "# Store toolgroups for agent creation later\n",
        "TOOLGROUPS = mcp_servers\n",
        "\n",
        "print(f\"üõ†Ô∏è MCP Servers: {len(mcp_servers)}\")\n",
        "print(f\"üìä Total Tools: {len(tools)}\")\n",
        "print(\"=\" * 50)\n",
        "for tg, tool_list in sorted(toolgroups.items()):\n",
        "    icon = \"üå§Ô∏è\" if \"weather\" in tg else \"üë•\" if \"hr\" in tg else \"üîß\"\n",
        "    print(f\"\\n{icon} {tg} ({len(tool_list)} tools)\")\n",
        "    for tool in tool_list:\n",
        "        print(f\"   ‚Ä¢ {tool}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Simple Chat Completion\n",
        "\n",
        "Let's test a basic chat completion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "payload = {\n",
        "    \"model\": MODEL_ID,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is the capital of France? Answer in one sentence.\"}\n",
        "    ],\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 256\n",
        "}\n",
        "\n",
        "print(f\"ü§ñ Using model: {MODEL_ID}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "response = requests.post(\n",
        "    f\"{LLAMASTACK_URL}/v1/openai/v1/chat/completions\",\n",
        "    json=payload,\n",
        "    timeout=60\n",
        ")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    content = result.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "    print(f\"\\nüìù Response:\")\n",
        "    print(content)\n",
        "else:\n",
        "    print(f\"‚ùå Error: {response.status_code} - {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üõ†Ô∏è Part 2: Using MCP Tools\n",
        "\n",
        "There are **two ways** to use MCP tools in LlamaStack:\n",
        "\n",
        "1. **Direct Tool Invocation** - Call tools directly via the `/v1/tool-runtime/invoke` API\n",
        "2. **Agent-Based Tool Calling** - Create an agent that automatically decides when to use tools\n",
        "\n",
        "Let's try both!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def invoke_tool(tool_name: str, kwargs: dict = None) -> str:\n",
        "    \"\"\"Invoke a tool directly via LlamaStack.\"\"\"\n",
        "    if kwargs is None:\n",
        "        kwargs = {}\n",
        "    \n",
        "    response = requests.post(\n",
        "        f\"{LLAMASTACK_URL}/v1/tool-runtime/invoke\",\n",
        "        json={\"tool_name\": tool_name, \"kwargs\": kwargs},\n",
        "        timeout=30\n",
        "    )\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        content = result.get(\"content\", [])\n",
        "        if isinstance(content, list) and content:\n",
        "            return content[0].get(\"text\", str(content))\n",
        "        return str(result)\n",
        "    else:\n",
        "        return f\"Error: {response.status_code} - {response.text}\"\n",
        "\n",
        "print(\"‚úÖ invoke_tool function ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Direct Tool Invocation\n",
        "\n",
        "You can call MCP tools directly without involving the LLM. This is useful for testing tools or when you know exactly which tool you need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Weather MCP - Get statistics\n",
        "print(\"üå§Ô∏è Weather MCP - Get Statistics\")\n",
        "print(\"=\" * 50)\n",
        "result = invoke_tool(\"get_weather_statistics\")\n",
        "print(result[:1000] if len(result) > 1000 else result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Weather MCP - List stations\n",
        "print(\"üå§Ô∏è Weather MCP - List Stations\")\n",
        "print(\"=\" * 50)\n",
        "result = invoke_tool(\"list_weather_stations\")\n",
        "print(result[:1500] if len(result) > 1500 else result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Agent-Based Tool Calling\n",
        "\n",
        "The more powerful approach is to create an **Agent** that can automatically decide when to use tools based on the user's question.\n",
        "\n",
        "This uses the LlamaStack Agents API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Create an agent with MCP tools enabled\n",
        "print(\"ü§ñ Creating Agent with MCP Tools...\")\n",
        "print(f\"   Model: {MODEL_ID}\")\n",
        "print(f\"   Toolgroups: {TOOLGROUPS}\")\n",
        "\n",
        "agent_config = {\n",
        "    \"agent_config\": {\n",
        "        \"model\": MODEL_ID,\n",
        "        \"instructions\": \"You are a helpful assistant. Use the available tools to answer questions about weather and other data.\",\n",
        "        \"toolgroups\": TOOLGROUPS,\n",
        "        \"enable_session_persistence\": False,\n",
        "        \"sampling_params\": {\n",
        "            \"max_tokens\": 1024,\n",
        "            \"temperature\": 0.7\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "response = requests.post(f\"{LLAMASTACK_URL}/v1/agents\", json=agent_config, timeout=30)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    agent_id = response.json().get(\"agent_id\")\n",
        "    print(f\"\\n‚úÖ Agent created: {agent_id}\")\n",
        "else:\n",
        "    print(f\"‚ùå Error creating agent: {response.status_code} - {response.text}\")\n",
        "    agent_id = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Create a session for the agent\n",
        "if agent_id:\n",
        "    session_response = requests.post(\n",
        "        f\"{LLAMASTACK_URL}/v1/agents/{agent_id}/session\",\n",
        "        json={\"session_name\": \"workshop-session\"},\n",
        "        timeout=30\n",
        "    )\n",
        "    \n",
        "    if session_response.status_code == 200:\n",
        "        session_id = session_response.json().get(\"session_id\")\n",
        "        print(f\"‚úÖ Session created: {session_id}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Error creating session: {session_response.status_code}\")\n",
        "        session_id = None\n",
        "else:\n",
        "    session_id = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_agent(question: str):\n",
        "    \"\"\"Ask the agent a question - it will use tools automatically!\"\"\"\n",
        "    if not agent_id or not session_id:\n",
        "        print(\"‚ùå Agent or session not created. Run the cells above first.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"‚ùì Question: {question}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Note: LlamaStack Agents API requires stream=True\n",
        "    turn_request = {\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": question}],\n",
        "        \"stream\": True\n",
        "    }\n",
        "    \n",
        "    response = requests.post(\n",
        "        f\"{LLAMASTACK_URL}/v1/agents/{agent_id}/session/{session_id}/turn\",\n",
        "        json=turn_request,\n",
        "        timeout=120,\n",
        "        stream=True\n",
        "    )\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        tool_calls = []\n",
        "        final_response = \"\"\n",
        "        \n",
        "        # Parse streaming response (Server-Sent Events format)\n",
        "        for line in response.iter_lines():\n",
        "            if line:\n",
        "                line_str = line.decode('utf-8')\n",
        "                if line_str.startswith('data: '):\n",
        "                    try:\n",
        "                        data = json.loads(line_str[6:])\n",
        "                        event = data.get(\"event\", {}).get(\"payload\", {})\n",
        "                        event_type = event.get(\"event_type\", \"\")\n",
        "                        \n",
        "                        # Capture tool calls\n",
        "                        if event_type == \"step_complete\" and event.get(\"step_type\") == \"tool_execution\":\n",
        "                            step_details = event.get(\"step_details\", {})\n",
        "                            for tc in step_details.get(\"tool_calls\", []):\n",
        "                                tool_calls.append(tc.get(\"tool_name\", \"unknown\"))\n",
        "                        \n",
        "                        # Capture final response\n",
        "                        if event_type == \"turn_complete\":\n",
        "                            turn = event.get(\"turn\", {})\n",
        "                            for msg in turn.get(\"output_message\", {}).get(\"content\", []):\n",
        "                                if msg.get(\"type\") == \"text\":\n",
        "                                    final_response = msg.get(\"text\", \"\")\n",
        "                    except json.JSONDecodeError:\n",
        "                        pass\n",
        "        \n",
        "        # Show results\n",
        "        if tool_calls:\n",
        "            print(\"\\nüîß Tools Used:\")\n",
        "            for tc in tool_calls:\n",
        "                print(f\"   ‚Ä¢ {tc}\")\n",
        "        \n",
        "        if final_response:\n",
        "            print(f\"\\nüìù Response:\")\n",
        "            print(final_response)\n",
        "        elif not tool_calls:\n",
        "            print(\"‚ö†Ô∏è No response received\")\n",
        "    else:\n",
        "        print(f\"‚ùå Error: {response.status_code} - {response.text}\")\n",
        "\n",
        "print(\"‚úÖ ask_agent function ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test: Ask about weather stations\n",
        "ask_agent(\"List all available weather stations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test: Get weather statistics\n",
        "ask_agent(\"Get weather statistics\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Explore on Your Own!\n",
        "\n",
        "Try different questions. The agent will automatically use the appropriate MCP tools.\n",
        "\n",
        "**Weather questions:**\n",
        "- \"List all available weather stations\"\n",
        "- \"Get weather statistics\"\n",
        "- \"Search for weather observations in New Delhi\"\n",
        "- \"Get current weather for station VIDP\"\n",
        "\n",
        "**Station codes:** VIDP = New Delhi, RJTT = Tokyo, KJFK = New York, EGLL = London, YSSY = Sydney\n",
        "\n",
        "**HR questions (if HR MCP is configured):**\n",
        "- \"List all employees\"\n",
        "- \"Get vacation balance for employee EMP001\"\n",
        "- \"List all job openings\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your turn! Try your own questions\n",
        "my_question = \"Get weather statistics\"  # <-- Change this!\n",
        "\n",
        "ask_agent(my_question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üéì Workshop Complete!\n",
        "\n",
        "## What You Learned\n",
        "\n",
        "1. ‚úÖ **List Models** - Query available LLM models\n",
        "2. ‚úÖ **List Tools** - Discover MCP servers and their tools\n",
        "3. ‚úÖ **Chat Completion** - Basic conversation with the model\n",
        "4. ‚úÖ **Direct Tool Invocation** - Call MCP tools directly via API\n",
        "5. ‚úÖ **Agent-Based Tool Calling** - Let the AI decide when to use tools\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "- **MCP Tools are Unified**: All tools are accessible through the same API\n",
        "- **Two Ways to Use Tools**: Direct invocation for specific needs, Agent API for automatic tool selection\n",
        "- **Easy to Extend**: Just add more MCP servers to your LlamaStack config to give your AI new capabilities!"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
