{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LlamaStack Workshop - Client Demo\n",
        "\n",
        "This notebook demonstrates how to interact with your deployed LlamaStack distribution programmatically.\n",
        "\n",
        "## Prerequisites\n",
        "- You have deployed a model (Llama 3.2-3B) in your project\n",
        "- You have enabled the GenAI Playground (which creates a LlamaStack Distribution)\n",
        "\n",
        "## What You'll Learn\n",
        "1. How to list available models\n",
        "2. How to list available tools (MCP servers)\n",
        "3. How to make chat completions\n",
        "4. How to use tool calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Configuration - UPDATE THIS with your project name!\n",
        "PROJECT_NAME = \"user-XX\"  # <-- Change XX to your user number (e.g., user-05)\n",
        "\n",
        "# LlamaStack endpoint (internal OpenShift service)\n",
        "LLAMASTACK_URL = f\"http://lsd-genai-playground-service.{PROJECT_NAME}.svc.cluster.local:8321\"\n",
        "\n",
        "print(f\"Project: {PROJECT_NAME}\")\n",
        "print(f\"LlamaStack URL: {LLAMASTACK_URL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. List Available Models\n",
        "\n",
        "Let's see what models are available in your LlamaStack distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = requests.get(f\"{LLAMASTACK_URL}/v1/models\", timeout=10)\n",
        "models = response.json().get(\"data\", [])\n",
        "\n",
        "# Filter to LLM models only\n",
        "llm_models = [m for m in models if m.get(\"model_type\") == \"llm\"]\n",
        "\n",
        "print(f\"ðŸ¤– LLM Models Available: {len(llm_models)}\")\n",
        "print(\"=\" * 50)\n",
        "for m in llm_models:\n",
        "    print(f\"  â€¢ {m.get('identifier')} ({m.get('provider_id')})\")\n",
        "    \n",
        "# Store the first model ID for later use\n",
        "# Note: identifier already includes the provider prefix (e.g., \"vllm-inference-1/llama-32-3b-instruct\")\n",
        "if llm_models:\n",
        "    MODEL_ID = llm_models[0].get('identifier')\n",
        "    print(f\"\\nðŸ“Œ Using model: {MODEL_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. List Available Tools (MCP Servers)\n",
        "\n",
        "Tools are provided by MCP servers. Let's see what's available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = requests.get(f\"{LLAMASTACK_URL}/v1/tools\", timeout=10)\n",
        "data = response.json()\n",
        "tools = data if isinstance(data, list) else data.get(\"data\", [])\n",
        "\n",
        "# Group by toolgroup (MCP server)\n",
        "toolgroups = {}\n",
        "for t in tools:\n",
        "    tg = t.get(\"toolgroup_id\", \"unknown\")\n",
        "    if tg not in toolgroups:\n",
        "        toolgroups[tg] = []\n",
        "    toolgroups[tg].append(t.get(\"name\", \"unknown\"))\n",
        "\n",
        "# Count MCP servers (exclude builtin)\n",
        "mcp_servers = [tg for tg in toolgroups.keys() if tg.startswith(\"mcp::\")]\n",
        "\n",
        "# Store toolgroup IDs for later use\n",
        "TOOLGROUPS = [tg for tg in toolgroups.keys() if tg.startswith(\"mcp::\")]\n",
        "\n",
        "print(f\"ðŸ› ï¸ MCP Servers: {len(mcp_servers)}\")\n",
        "print(f\"ðŸ“Š Total Tools: {len(tools)}\")\n",
        "print(\"=\" * 50)\n",
        "for tg, tool_list in sorted(toolgroups.items()):\n",
        "    icon = \"ðŸŒ¤ï¸\" if \"weather\" in tg else \"ðŸ”§\"\n",
        "    print(f\"\\n{icon} {tg} ({len(tool_list)} tools)\")\n",
        "    for tool in tool_list:\n",
        "        print(f\"   â€¢ {tool}\")\n",
        "        \n",
        "print(f\"\\nðŸ“Œ Toolgroups for agent: {TOOLGROUPS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Simple Chat Completion\n",
        "\n",
        "Let's test a basic chat completion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "payload = {\n",
        "    \"model\": MODEL_ID,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is the capital of France? Answer in one sentence.\"}\n",
        "    ],\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 256\n",
        "}\n",
        "\n",
        "print(f\"ðŸ¤– Using model: {MODEL_ID}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "response = requests.post(\n",
        "    f\"{LLAMASTACK_URL}/v1/openai/v1/chat/completions\",\n",
        "    json=payload,\n",
        "    timeout=60\n",
        ")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    content = result.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "    print(f\"\\nðŸ“ Response:\")\n",
        "    print(content)\n",
        "else:\n",
        "    print(f\"âŒ Error: {response.status_code} - {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test Weather MCP Tool ðŸŒ¤ï¸\n",
        "\n",
        "Now let's test the Weather MCP tool using the LlamaStack Agents API, which properly handles tool calling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Create an agent with Weather MCP tools\n",
        "agent_config = {\n",
        "    \"agent_config\": {\n",
        "        \"model\": MODEL_ID,\n",
        "        \"instructions\": \"You are a helpful assistant. Use the available tools to answer questions.\",\n",
        "        \"toolgroups\": TOOLGROUPS,  # This enables the MCP tools!\n",
        "        \"enable_session_persistence\": False\n",
        "    }\n",
        "}\n",
        "\n",
        "response = requests.post(f\"{LLAMASTACK_URL}/v1/agents\", json=agent_config, timeout=30)\n",
        "if response.status_code == 200:\n",
        "    agent_id = response.json().get(\"agent_id\")\n",
        "    print(f\"âœ… Agent created: {agent_id}\")\n",
        "else:\n",
        "    print(f\"âŒ Error creating agent: {response.status_code} - {response.text}\")\n",
        "    agent_id = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create a session and ask a question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Create a session\n",
        "if agent_id:\n",
        "    session_response = requests.post(\n",
        "        f\"{LLAMASTACK_URL}/v1/agents/{agent_id}/session\",\n",
        "        json={\"session_name\": \"workshop-session\"},\n",
        "        timeout=30\n",
        "    )\n",
        "    if session_response.status_code == 200:\n",
        "        session_id = session_response.json().get(\"session_id\")\n",
        "        print(f\"âœ… Session created: {session_id}\")\n",
        "    else:\n",
        "        print(f\"âŒ Error creating session: {session_response.status_code}\")\n",
        "        session_id = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Ask a question using the Weather MCP tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3: Ask a question - the agent will use the Weather MCP tool!\n",
        "weather_question = \"List all available weather stations\"\n",
        "\n",
        "if agent_id and session_id:\n",
        "    print(f\"ðŸŒ¤ï¸ Weather Query: {weather_question}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    turn_request = {\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": weather_question}],\n",
        "        \"stream\": False\n",
        "    }\n",
        "    \n",
        "    response = requests.post(\n",
        "        f\"{LLAMASTACK_URL}/v1/agents/{agent_id}/session/{session_id}/turn\",\n",
        "        json=turn_request,\n",
        "        timeout=120\n",
        "    )\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        # Extract the assistant's response\n",
        "        for event in result.get(\"events\", []):\n",
        "            if event.get(\"event_type\") == \"turn_complete\":\n",
        "                turn = event.get(\"turn\", {})\n",
        "                for msg in turn.get(\"output_message\", {}).get(\"content\", []):\n",
        "                    if msg.get(\"type\") == \"text\":\n",
        "                        print(f\"\\nðŸ“ Response:\")\n",
        "                        print(msg.get(\"text\", \"\"))\n",
        "                # Show tool calls if any\n",
        "                for step in turn.get(\"steps\", []):\n",
        "                    if step.get(\"step_type\") == \"tool_execution\":\n",
        "                        print(f\"\\nðŸ”§ Tool called: {step.get('tool_calls', [])}\")\n",
        "    else:\n",
        "        print(f\"âŒ Error: {response.status_code} - {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Explore on Your Own!\n",
        "\n",
        "Try modifying the query below to test different questions. The agent will use the appropriate MCP tools.\n",
        "\n",
        "**Weather questions:**\n",
        "- \"Get weather statistics\"\n",
        "- \"Search for weather in Tokyo\"\n",
        "- \"Get current weather for station VIDP\"\n",
        "\n",
        "**HR questions (after Part 3):**\n",
        "- \"List all employees\"\n",
        "- \"Get vacation balance for employee EMP001\"\n",
        "- \"List all job openings\"\n",
        "\n",
        "Station codes: VIDP = New Delhi, RJTT = Tokyo, KJFK = New York, EGLL = London, YSSY = Sydney"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your turn! Modify this query to try different questions\n",
        "my_question = \"Get weather statistics\"  # Try changing this!\n",
        "\n",
        "if agent_id and session_id:\n",
        "    print(f\"ðŸ” Query: {my_question}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    turn_request = {\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": my_question}],\n",
        "        \"stream\": False\n",
        "    }\n",
        "    \n",
        "    response = requests.post(\n",
        "        f\"{LLAMASTACK_URL}/v1/agents/{agent_id}/session/{session_id}/turn\",\n",
        "        json=turn_request,\n",
        "        timeout=120\n",
        "    )\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        for event in result.get(\"events\", []):\n",
        "            if event.get(\"event_type\") == \"turn_complete\":\n",
        "                turn = event.get(\"turn\", {})\n",
        "                for msg in turn.get(\"output_message\", {}).get(\"content\", []):\n",
        "                    if msg.get(\"type\") == \"text\":\n",
        "                        print(f\"\\nðŸ“ Response:\")\n",
        "                        print(msg.get(\"text\", \"\"))\n",
        "    else:\n",
        "        print(f\"âŒ Error: {response.status_code} - {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸŽ¯ Part 3: After Adding HR MCP\n",
        "\n",
        "**Run this section AFTER you complete Part 3 of the workshop** (adding HR MCP to your LlamaStack config).\n",
        "\n",
        "First, restart the kernel and run all cells again to pick up the new HR tools. Then run the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test using BOTH MCP servers in one query!\n",
        "combined_question = \"List all weather stations and list all employees\"\n",
        "\n",
        "if agent_id and session_id:\n",
        "    print(f\"ðŸŒ¤ï¸ðŸ‘¥ Combined Query: {combined_question}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    turn_request = {\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": combined_question}],\n",
        "        \"stream\": False\n",
        "    }\n",
        "    \n",
        "    response = requests.post(\n",
        "        f\"{LLAMASTACK_URL}/v1/agents/{agent_id}/session/{session_id}/turn\",\n",
        "        json=turn_request,\n",
        "        timeout=180\n",
        "    )\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        for event in result.get(\"events\", []):\n",
        "            if event.get(\"event_type\") == \"turn_complete\":\n",
        "                turn = event.get(\"turn\", {})\n",
        "                for msg in turn.get(\"output_message\", {}).get(\"content\", []):\n",
        "                    if msg.get(\"type\") == \"text\":\n",
        "                        print(f\"\\nðŸ“ Response:\")\n",
        "                        print(msg.get(\"text\", \"\"))\n",
        "                # Check if tools were used\n",
        "                tool_steps = [s for s in turn.get(\"steps\", []) if s.get(\"step_type\") == \"tool_execution\"]\n",
        "                if tool_steps:\n",
        "                    print(\"\\n\" + \"=\" * 50)\n",
        "                    print(\"ðŸŽ‰ SUCCESS! Your AI used MCP tools!\")\n",
        "                    for step in tool_steps:\n",
        "                        for tc in step.get(\"tool_calls\", []):\n",
        "                            print(f\"   ðŸ”§ Called: {tc.get('tool_name')}\")\n",
        "    else:\n",
        "        print(f\"âŒ Error: {response.status_code} - {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ“ Workshop Complete!\n",
        "\n",
        "Congratulations! You've learned how to:\n",
        "- âœ… List available models in LlamaStack\n",
        "- âœ… List available tools (MCP servers)\n",
        "- âœ… Make chat completions\n",
        "- âœ… Use Weather MCP tools\n",
        "- âœ… Use HR MCP tools\n",
        "- âœ… Combine multiple MCP tools in one query\n",
        "\n",
        "**Key Takeaway:** You added new capabilities to your AI by just updating a configuration file - no coding required!"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
