{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LlamaStack Workshop - Client Demo\n",
        "\n",
        "This notebook demonstrates how to interact with your deployed LlamaStack distribution programmatically.\n",
        "\n",
        "## Prerequisites\n",
        "- You have deployed a model (Llama 3.2-3B) in your project\n",
        "- You have enabled the GenAI Playground (which creates a LlamaStack Distribution)\n",
        "\n",
        "## What You'll Learn\n",
        "1. How to list available models\n",
        "2. How to list available tools (MCP servers)\n",
        "3. How to make chat completions\n",
        "4. How to use tool calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Configuration - UPDATE THIS with your project name!\n",
        "PROJECT_NAME = \"user-XX\"  # <-- Change XX to your user number (e.g., user-05)\n",
        "\n",
        "# LlamaStack endpoint (internal OpenShift service)\n",
        "LLAMASTACK_URL = f\"http://lsd-genai-playground-service.{PROJECT_NAME}.svc.cluster.local:8321\"\n",
        "\n",
        "print(f\"Project: {PROJECT_NAME}\")\n",
        "print(f\"LlamaStack URL: {LLAMASTACK_URL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. List Available Models\n",
        "\n",
        "Let's see what models are available in your LlamaStack distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = requests.get(f\"{LLAMASTACK_URL}/v1/models\", timeout=10)\n",
        "models = response.json().get(\"data\", [])\n",
        "\n",
        "# Filter to LLM models only\n",
        "llm_models = [m for m in models if m.get(\"model_type\") == \"llm\"]\n",
        "\n",
        "print(f\"ðŸ¤– LLM Models Available: {len(llm_models)}\")\n",
        "print(\"=\" * 50)\n",
        "for m in llm_models:\n",
        "    print(f\"  â€¢ {m.get('identifier')} ({m.get('provider_id')})\")\n",
        "    \n",
        "# Store the first model ID for later use\n",
        "# Note: identifier already includes the provider prefix (e.g., \"vllm-inference-1/llama-32-3b-instruct\")\n",
        "if llm_models:\n",
        "    MODEL_ID = llm_models[0].get('identifier')\n",
        "    print(f\"\\nðŸ“Œ Using model: {MODEL_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. List Available Tools (MCP Servers)\n",
        "\n",
        "Tools are provided by MCP servers. Let's see what's available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = requests.get(f\"{LLAMASTACK_URL}/v1/tools\", timeout=10)\n",
        "data = response.json()\n",
        "tools = data if isinstance(data, list) else data.get(\"data\", [])\n",
        "\n",
        "# Group by toolgroup (MCP server)\n",
        "toolgroups = {}\n",
        "for t in tools:\n",
        "    tg = t.get(\"toolgroup_id\", \"unknown\")\n",
        "    if tg not in toolgroups:\n",
        "        toolgroups[tg] = []\n",
        "    toolgroups[tg].append(t.get(\"name\", \"unknown\"))\n",
        "\n",
        "# Count MCP servers (exclude builtin)\n",
        "mcp_servers = [tg for tg in toolgroups.keys() if tg.startswith(\"mcp::\")]\n",
        "\n",
        "print(f\"ðŸ› ï¸ MCP Servers: {len(mcp_servers)}\")\n",
        "print(f\"ðŸ“Š Total Tools: {len(tools)}\")\n",
        "print(\"=\" * 50)\n",
        "for tg, tool_list in sorted(toolgroups.items()):\n",
        "    icon = \"ðŸŒ¤ï¸\" if \"weather\" in tg else \"ðŸ”§\"\n",
        "    print(f\"\\n{icon} {tg} ({len(tool_list)} tools)\")\n",
        "    for tool in tool_list:\n",
        "        print(f\"   â€¢ {tool}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Simple Chat Completion\n",
        "\n",
        "Let's test a basic chat completion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "payload = {\n",
        "    \"model\": MODEL_ID,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is the capital of France? Answer in one sentence.\"}\n",
        "    ],\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 256\n",
        "}\n",
        "\n",
        "print(f\"ðŸ¤– Using model: {MODEL_ID}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "response = requests.post(\n",
        "    f\"{LLAMASTACK_URL}/v1/openai/v1/chat/completions\",\n",
        "    json=payload,\n",
        "    timeout=60\n",
        ")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    content = result.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "    print(f\"\\nðŸ“ Response:\")\n",
        "    print(content)\n",
        "else:\n",
        "    print(f\"âŒ Error: {response.status_code} - {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Explore on Your Own!\n",
        "\n",
        "Try modifying the queries to ask different questions or experiment with temperature settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Weather MCP - List all weather stations\n",
        "weather_question = \"List all available weather stations\"\n",
        "\n",
        "payload = {\n",
        "    \"model\": MODEL_ID,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": weather_question}\n",
        "    ],\n",
        "    \"temperature\": 0.1,  # Lower temperature for more consistent tool usage\n",
        "    \"max_tokens\": 1024\n",
        "}\n",
        "\n",
        "print(f\"ðŸŒ¤ï¸ Weather Query: {weather_question}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "response = requests.post(\n",
        "    f\"{LLAMASTACK_URL}/v1/openai/v1/chat/completions\",\n",
        "    json=payload,\n",
        "    timeout=120  # Longer timeout for tool calls\n",
        ")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    content = result.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "    print(f\"\\nðŸ“ Response:\")\n",
        "    print(content)\n",
        "else:\n",
        "    print(f\"âŒ Error: {response.status_code} - {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Explore on Your Own!\n",
        "\n",
        "Try modifying the query below to test different weather questions:\n",
        "- \"Get weather statistics\"\n",
        "- \"Search for weather in Tokyo\"\n",
        "- \"Get current weather for station VIDP\"\n",
        "\n",
        "Station codes: VIDP = New Delhi, RJTT = Tokyo, KJFK = New York, EGLL = London, YSSY = Sydney"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your turn! Modify this query to try different weather questions\n",
        "my_question = \"Get weather statistics\"\n",
        "\n",
        "payload = {\n",
        "    \"model\": MODEL_ID,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": my_question}\n",
        "    ],\n",
        "    \"temperature\": 0.1,\n",
        "    \"max_tokens\": 1024\n",
        "}\n",
        "\n",
        "print(f\"ðŸ” Query: {my_question}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "response = requests.post(\n",
        "    f\"{LLAMASTACK_URL}/v1/openai/v1/chat/completions\",\n",
        "    json=payload,\n",
        "    timeout=120\n",
        ")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    content = result.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "    print(f\"\\nðŸ“ Response:\")\n",
        "    print(content)\n",
        "else:\n",
        "    print(f\"âŒ Error: {response.status_code} - {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸŽ¯ Part 3: After Adding HR MCP\n",
        "\n",
        "**Run this section AFTER you complete Part 3 of the workshop** (adding HR MCP to your LlamaStack config).\n",
        "\n",
        "First, let's verify that HR tools are now available:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Re-check available tools - you should now see HR tools!\n",
        "response = requests.get(f\"{LLAMASTACK_URL}/v1/tools\", timeout=10)\n",
        "data = response.json()\n",
        "tools = data if isinstance(data, list) else data.get(\"data\", [])\n",
        "\n",
        "# Group by toolgroup (MCP server)\n",
        "toolgroups = {}\n",
        "for t in tools:\n",
        "    tg = t.get(\"toolgroup_id\", \"unknown\")\n",
        "    if tg not in toolgroups:\n",
        "        toolgroups[tg] = []\n",
        "    toolgroups[tg].append(t.get(\"name\", \"unknown\"))\n",
        "\n",
        "# Count MCP servers (exclude builtin)\n",
        "mcp_servers = [tg for tg in toolgroups.keys() if tg.startswith(\"mcp::\")]\n",
        "\n",
        "print(f\"ðŸ› ï¸ MCP Servers: {len(mcp_servers)}\")\n",
        "print(f\"ðŸ“Š Total Tools: {len(tools)}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check if HR tools are present\n",
        "has_hr = any(\"hr\" in tg.lower() for tg in toolgroups.keys())\n",
        "has_weather = any(\"weather\" in tg.lower() for tg in toolgroups.keys())\n",
        "\n",
        "if has_hr and has_weather:\n",
        "    print(\"âœ… SUCCESS! Both Weather AND HR MCP servers are connected!\")\n",
        "elif has_weather:\n",
        "    print(\"âš ï¸ Only Weather MCP is connected. Did you complete Part 3?\")\n",
        "else:\n",
        "    print(\"âŒ No MCP servers found. Check your LlamaStack config.\")\n",
        "\n",
        "print(\"\")\n",
        "for tg, tool_list in sorted(toolgroups.items()):\n",
        "    if \"weather\" in tg.lower():\n",
        "        icon = \"ðŸŒ¤ï¸\"\n",
        "    elif \"hr\" in tg.lower():\n",
        "        icon = \"ðŸ‘¥\"\n",
        "    else:\n",
        "        icon = \"ðŸ”§\"\n",
        "    print(f\"\\n{icon} {tg} ({len(tool_list)} tools)\")\n",
        "    for tool in tool_list:\n",
        "        print(f\"   â€¢ {tool}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test HR MCP Tool\n",
        "\n",
        "Now let's test the HR MCP tool!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test HR MCP - List all employees\n",
        "hr_question = \"List all employees\"\n",
        "\n",
        "payload = {\n",
        "    \"model\": MODEL_ID,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": hr_question}\n",
        "    ],\n",
        "    \"temperature\": 0.1,\n",
        "    \"max_tokens\": 1024\n",
        "}\n",
        "\n",
        "print(f\"ðŸ‘¥ HR Query: {hr_question}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "response = requests.post(\n",
        "    f\"{LLAMASTACK_URL}/v1/openai/v1/chat/completions\",\n",
        "    json=payload,\n",
        "    timeout=120\n",
        ")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    content = result.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "    print(f\"\\nðŸ“ Response:\")\n",
        "    print(content)\n",
        "else:\n",
        "    print(f\"âŒ Error: {response.status_code} - {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Use BOTH Tools Together! ðŸŽ‰\n",
        "\n",
        "The real power of LlamaStack is using multiple MCP servers together. Let's ask a question that requires BOTH Weather AND HR tools:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use BOTH MCP servers in one query!\n",
        "combined_question = \"List all weather stations and list all employees\"\n",
        "\n",
        "payload = {\n",
        "    \"model\": MODEL_ID,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": combined_question}\n",
        "    ],\n",
        "    \"temperature\": 0.1,\n",
        "    \"max_tokens\": 2048\n",
        "}\n",
        "\n",
        "print(f\"ðŸŒ¤ï¸ðŸ‘¥ Combined Query: {combined_question}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "response = requests.post(\n",
        "    f\"{LLAMASTACK_URL}/v1/openai/v1/chat/completions\",\n",
        "    json=payload,\n",
        "    timeout=180  # Even longer timeout for multiple tool calls\n",
        ")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    content = result.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "    print(f\"\\nðŸ“ Response:\")\n",
        "    print(content)\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"ðŸŽ‰ SUCCESS! Your AI used BOTH Weather AND HR tools!\")\n",
        "else:\n",
        "    print(f\"âŒ Error: {response.status_code} - {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ“ Workshop Complete!\n",
        "\n",
        "Congratulations! You've learned how to:\n",
        "- âœ… List available models in LlamaStack\n",
        "- âœ… List available tools (MCP servers)\n",
        "- âœ… Make chat completions\n",
        "- âœ… Use Weather MCP tools\n",
        "- âœ… Use HR MCP tools\n",
        "- âœ… Combine multiple MCP tools in one query\n",
        "\n",
        "**Key Takeaway:** You added new capabilities to your AI by just updating a configuration file - no coding required!"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
