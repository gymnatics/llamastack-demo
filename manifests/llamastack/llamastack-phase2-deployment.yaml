# LlamaStack Phase 2 - Separate Deployment
# This is a SEPARATE LlamaStack distribution with Azure OpenAI support
# 
# Deploy alongside the existing lsd-genai-playground (Phase 1)
#
# Usage:
#   oc apply -f manifests/llamastack/llamastack-phase2-deployment.yaml
#
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-stack-config-phase2
  namespace: my-first-model
data:
  run.yaml: |
    version: "2"
    image_name: rh
    apis:
    - agents
    - datasetio
    - files
    - inference
    - safety
    - scoring
    - tool_runtime
    - vector_io
    providers:
      inference:
      - provider_id: sentence-transformers
        provider_type: inline::sentence-transformers
        config: {}
      # Provider 1: vLLM (local)
      - provider_id: vllm-inference
        provider_type: remote::vllm
        config:
          api_token: ${env.VLLM_API_TOKEN:=fake}
          max_tokens: ${env.VLLM_MAX_TOKENS:=4096}
          tls_verify: ${env.VLLM_TLS_VERIFY:=true}
          url: http://llama-32-3b-instruct-predictor.my-first-model.svc.cluster.local:8080/v1
      # Provider 2: Azure OpenAI
      - provider_id: azure-openai
        provider_type: remote::azure
        config:
          api_base: ${env.AZURE_OPENAI_ENDPOINT}
          api_key: ${env.AZURE_OPENAI_API_KEY}
          api_version: ${env.AZURE_OPENAI_API_VERSION:=2024-12-01-preview}
      vector_io:
      - provider_id: milvus
        provider_type: inline::milvus
        config:
          db_path: /opt/app-root/src/.llama/distributions/rh/milvus.db
          kvstore:
            db_path: /opt/app-root/src/.llama/distributions/rh/milvus_registry.db
            namespace: null
            type: sqlite
      agents:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          persistence_store:
            db_path: /opt/app-root/src/.llama/distributions/rh/agents_store.db
            namespace: null
            type: sqlite
          responses_store:
            db_path: /opt/app-root/src/.llama/distributions/rh/responses_store.db
            type: sqlite
      eval: []
      files:
      - provider_id: meta-reference-files
        provider_type: inline::localfs
        config:
          metadata_store:
            db_path: /opt/app-root/src/.llama/distributions/rh/files_metadata.db
            type: sqlite
          storage_dir: /opt/app-root/src/.llama/distributions/rh/files
      datasetio:
      - provider_id: huggingface
        provider_type: remote::huggingface
        config:
          kvstore:
            db_path: /opt/app-root/src/.llama/distributions/rh/huggingface_datasetio.db
            namespace: null
            type: sqlite
      scoring:
      - provider_id: basic
        provider_type: inline::basic
        config: {}
      - provider_id: llm-as-judge
        provider_type: inline::llm-as-judge
        config: {}
      tool_runtime:
      - provider_id: rag-runtime
        provider_type: inline::rag-runtime
        config: {}
      - provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
        config: {}
    metadata_store:
      type: sqlite
      db_path: /opt/app-root/src/.llama/distributions/rh/inference_store.db
    models:
    - provider_id: sentence-transformers
      model_id: granite-embedding-125m
      provider_model_id: ibm-granite/granite-embedding-125m-english
      model_type: embedding
      metadata:
        embedding_dimension: 768
    # Model 1: Local vLLM
    - provider_id: vllm-inference
      model_id: llama-32-3b-instruct
      model_type: llm
      metadata:
        description: "Local Llama 3.2-3B via vLLM"
        display_name: llama-32-3b-instruct
    # Model 2: Azure OpenAI
    - provider_id: azure-openai
      model_id: gpt-4.1-mini
      provider_model_id: gpt-4.1-mini
      model_type: llm
      metadata:
        description: "Azure OpenAI GPT-4.1 Mini"
        display_name: gpt-4.1-mini (Azure)
    shields: []
    vector_dbs: []
    datasets: []
    scoring_fns: []
    benchmarks: []
    tool_groups:
    - toolgroup_id: builtin::rag
      provider_id: rag-runtime
    # MCP Server 1: Weather
    - toolgroup_id: mcp::weather-data
      provider_id: model-context-protocol
      mcp_endpoint:
        uri: http://mcp-weather.my-first-model.svc.cluster.local:80/sse
    # MCP Server 2: HR
    - toolgroup_id: mcp::hr-tools
      provider_id: model-context-protocol
      mcp_endpoint:
        uri: http://hr-mcp-server.my-first-model.svc.cluster.local:8000/mcp
    # MCP Server 3: Jira
    - toolgroup_id: mcp::jira-confluence
      provider_id: model-context-protocol
      mcp_endpoint:
        uri: http://jira-mcp-server.my-first-model.svc.cluster.local:8000/mcp
    server:
      port: 8321
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llamastack-phase2
  namespace: my-first-model
  labels:
    app: llamastack-phase2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llamastack-phase2
  template:
    metadata:
      labels:
        app: llamastack-phase2
    spec:
      initContainers:
      - name: ca-bundle-init
        image: registry.access.redhat.com/ubi9/ubi-minimal:latest
        command:
        - sh
        - -c
        - |
          cp /etc/pki/tls/certs/ca-bundle.crt /tmp/ca-bundle/ca-bundle.crt || true
          if [ -f /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem ]; then
            cat /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem >> /tmp/ca-bundle/ca-bundle.crt
          fi
        volumeMounts:
        - name: ca-bundle
          mountPath: /tmp/ca-bundle
      containers:
      - name: llamastack
        image: quay.io/ai-on-openshift/llama-stack-rhoai-ubi9:0.2.2
        ports:
        - containerPort: 8321
        env:
        - name: INFERENCE_MODEL
          value: "llama-32-3b-instruct"
        - name: VLLM_URL
          value: "http://llama-32-3b-instruct-predictor.my-first-model.svc.cluster.local:8080/v1"
        - name: VLLM_API_TOKEN
          value: "fake"
        - name: VLLM_TLS_VERIFY
          value: "false"
        - name: VLLM_MAX_TOKENS
          value: "4096"
        - name: SSL_CERT_FILE
          value: "/tmp/ca-bundle/ca-bundle.crt"
        - name: REQUESTS_CA_BUNDLE
          value: "/tmp/ca-bundle/ca-bundle.crt"
        # Azure OpenAI credentials from secret
        - name: AZURE_OPENAI_ENDPOINT
          valueFrom:
            secretKeyRef:
              name: azure-openai-secret
              key: endpoint
        - name: AZURE_OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: azure-openai-secret
              key: api-key
        - name: AZURE_OPENAI_API_VERSION
          valueFrom:
            secretKeyRef:
              name: azure-openai-secret
              key: api-version
        volumeMounts:
        - name: config
          mountPath: /app/config
        - name: ca-bundle
          mountPath: /tmp/ca-bundle
        command:
        - python
        - -m
        - llama_stack.distribution.server
        - --config
        - /app/config/run.yaml
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2"
      volumes:
      - name: config
        configMap:
          name: llama-stack-config-phase2
      - name: ca-bundle
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: llamastack-phase2-service
  namespace: my-first-model
spec:
  selector:
    app: llamastack-phase2
  ports:
  - port: 8321
    targetPort: 8321
  type: ClusterIP
